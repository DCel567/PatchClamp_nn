{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.layers import Attention\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.stats import shapiro\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, AveragePooling1D, Flatten, Activation, Conv2D, BatchNormalization, Input, Concatenate, Dense, MaxPooling1D, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data_extracted/1.csv'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "\n",
    "features = [1]\n",
    "\n",
    "for x in range(1, 11):\n",
    "    lag_name = \"lag_\" + str(x)\n",
    "    data[lag_name] = data[1].shift(x)\n",
    "    features.append(lag_name)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "time = data[0]\n",
    "y = data[2].astype(int)\n",
    "X = data[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                1     lag_1     lag_2     lag_3     lag_4     lag_5     lag_6  \\\n",
       "10     -0.192859 -0.111333 -0.043488  0.004215  0.027046  0.022390 -0.010066   \n",
       "11     -0.280377 -0.192859 -0.111333 -0.043488  0.004215  0.027046  0.022390   \n",
       "12     -0.365584 -0.280377 -0.192859 -0.111333 -0.043488  0.004215  0.027046   \n",
       "13     -0.440217 -0.365584 -0.280377 -0.192859 -0.111333 -0.043488  0.004215   \n",
       "14     -0.496707 -0.440217 -0.365584 -0.280377 -0.192859 -0.111333 -0.043488   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "129994  0.126695  0.141051  0.145359  0.142650  0.136523  0.130716  0.128570   \n",
       "129995  0.100302  0.126695  0.141051  0.145359  0.142650  0.136523  0.130716   \n",
       "129996  0.061114  0.100302  0.126695  0.141051  0.145359  0.142650  0.136523   \n",
       "129997  0.009739  0.061114  0.100302  0.126695  0.141051  0.145359  0.142650   \n",
       "129998 -0.051950  0.009739  0.061114  0.100302  0.126695  0.141051  0.145359   \n",
       "\n",
       "           lag_7     lag_8     lag_9    lag_10  \n",
       "10     -0.068362 -0.148402 -0.244349 -0.349114  \n",
       "11     -0.010066 -0.068362 -0.148402 -0.244349  \n",
       "12      0.022390 -0.010066 -0.068362 -0.148402  \n",
       "13      0.027046  0.022390 -0.010066 -0.068362  \n",
       "14      0.004215  0.027046  0.022390 -0.010066  \n",
       "...          ...       ...       ...       ...  \n",
       "129994  0.132604  0.144198  0.163433  0.189099  \n",
       "129995  0.128570  0.132604  0.144198  0.163433  \n",
       "129996  0.130716  0.128570  0.132604  0.144198  \n",
       "129997  0.136523  0.130716  0.128570  0.132604  \n",
       "129998  0.142650  0.136523  0.130716  0.128570  \n",
       "\n",
       "[129989 rows x 11 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "#y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Normalize the signal values\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "#X_with_time = np.column_stack((time, X))\n",
    "\n",
    "#X_scaled = X\n",
    "# Сначала разделяем данные на тренировочные и временные выборки\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Затем разделяем временные выборки на тестовые и валидационные\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Затем разделяем временные выборки на тестовые и валидационные\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding lag columns can help in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.flatten()\n",
    "X_test = X_test.flatten()\n",
    "X_val = X_val.flatten()\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "y_val = y_val.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103999"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103991, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, TimeDistributed, Bidirectional\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None, 1))\n",
    "encoder = Bidirectional(LSTM(64, return_sequences=True))(encoder_inputs)\n",
    "encoder = Dropout(0.2)(encoder)\n",
    "encoder_outputs, state_h, state_c = LSTM(64, return_state=True)(encoder)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None, 3))  # 3 classes: 0, 1, 2\n",
    "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_outputs = Dropout(0.2)(decoder_outputs)\n",
    "decoder_dense = TimeDistributed(Dense(3, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Labels: 10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "Name: 2, dtype: int32\n",
      "Decoder Inputs (shifted): [0 0 0 0 0 0 0 0 0 0]\n",
      "Decoder Inputs (one-hot encoded):\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Number of classes (0, 1, 2)\n",
    "num_classes = 3\n",
    "\n",
    "# Create decoder inputs by shifting the target labels\n",
    "# Add a start token (e.g., 0) at the beginning of the sequence\n",
    "start_token = 0  # You can use a special token if needed\n",
    "decoder_inputs = np.insert(y_train[:-1], 0, start_token)\n",
    "\n",
    "# One-hot encode the decoder inputs\n",
    "decoder_inputs_one_hot = np.eye(num_classes)[decoder_inputs]\n",
    "\n",
    "print(\"Target Labels:\", y_train[:10])\n",
    "print(\"Decoder Inputs (shifted):\", decoder_inputs[:10])\n",
    "print(\"Decoder Inputs (one-hot encoded):\")\n",
    "print(decoder_inputs_one_hot[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10   -0.192859\n",
      "11   -0.280377\n",
      "12   -0.365584\n",
      "13   -0.440217\n",
      "14   -0.496707\n",
      "15   -0.528726\n",
      "16   -0.531687\n",
      "17   -0.503070\n",
      "18   -0.442609\n",
      "19   -0.352334\n",
      "Name: 1, dtype: float64\n",
      "(103991,)\n"
     ]
    }
   ],
   "source": [
    "X_train_enc_dec = X_train[1]\n",
    "print(X_train_enc_dec[:10])\n",
    "print(X_train_enc_dec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103991,)\n",
      "(103991, 3)\n",
      "(103991,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_enc_dec.shape)\n",
    "print(decoder_inputs_one_hot.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m decoder_target_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(y_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (1, 50, 3)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_target_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\egrka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\egrka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1785\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1782\u001b[0m split_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_dim \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_split)))\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m batch_dim:\n\u001b[1;32m-> 1785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data contains \u001b[39m\u001b[38;5;132;01m{batch_dim}\u001b[39;00m\u001b[38;5;124m samples, which is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1787\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msufficient to split it into a validation and training set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified by `validation_split=\u001b[39m\u001b[38;5;132;01m{validation_split}\u001b[39;00m\u001b[38;5;124m`. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide more data, or a different value for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1791\u001b[0m             batch_dim\u001b[38;5;241m=\u001b[39mbatch_dim, validation_split\u001b[38;5;241m=\u001b[39mvalidation_split\n\u001b[0;32m   1792\u001b[0m         )\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split\u001b[39m(t, start, end):\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "# Add batch dimension\n",
    "encoder_input_data = np.expand_dims(X_train_enc_dec, axis=0)  # Shape: (1, 50, 1)\n",
    "decoder_input_data = np.expand_dims(decoder_inputs_one_hot, axis=0)  # Shape: (1, 50, 3)\n",
    "decoder_target_data = np.expand_dims(y_train, axis=0)  # Shape: (1, 50, 3)\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=1, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape: (1, 103991)\n",
      "Decoder input shape: (1, 103991, 3)\n",
      "Decoder target shape: (1, 103991)\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder input shape:\", encoder_input_data.shape)  # Should be (batch_size, sequence_length, 1)\n",
    "print(\"Decoder input shape:\", decoder_input_data.shape)  # Should be (batch_size, sequence_length, 3)\n",
    "print(\"Decoder target shape:\", decoder_target_data.shape)  # Should be (batch_size, sequence_length, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
